name: Database Backup

on:
  schedule:
    # Daily backup at 2 AM UTC
    - cron: '0 2 * * *'

  # Allow manual triggering
  workflow_dispatch:
    inputs:
      upload:
        description: 'Upload backup to cloud storage'
        required: false
        type: boolean
        default: true
      compress:
        description: 'Compress backup'
        required: false
        type: boolean
        default: true

env:
  BACKUP_DIR: ./backups

jobs:
  backup-database:
    name: Backup MongoDB Database
    runs-on: ubuntu-latest
    # Only run on main branch or manual trigger
    if: github.ref == 'refs/heads/main' || github.event_name == 'workflow_dispatch'

    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Setup MongoDB Tools
        run: |
          # Install MongoDB database tools (mongodump)
          wget -qO - https://www.mongodb.org/static/pgp/server-7.0.asc | sudo apt-key add -
          echo "deb [ arch=amd64,arm64 ] https://repo.mongodb.org/apt/ubuntu jammy/mongodb-org/7.0 multiverse" | sudo tee /etc/apt/sources.list.d/mongodb-org-7.0.list
          sudo apt-get update
          sudo apt-get install -y mongodb-database-tools

      - name: Create backup directory
        run: mkdir -p ${{ env.BACKUP_DIR }}

      - name: Set backup timestamp
        id: timestamp
        run: echo "timestamp=$(date +%Y%m%d_%H%M%S)" >> $GITHUB_OUTPUT

      - name: Run backup
        env:
          MONGODB_URI: ${{ secrets.MONGODB_URI }}
        run: |
          BACKUP_NAME="backup-production-${{ steps.timestamp.outputs.timestamp }}"
          BACKUP_PATH="${{ env.BACKUP_DIR }}/${BACKUP_NAME}"

          echo "Creating backup: $BACKUP_NAME"

          mongodump --uri="${MONGODB_URI}" --out="$BACKUP_PATH"

          if [ $? -eq 0 ]; then
            echo "✓ Backup created successfully"
            echo "backup_path=$BACKUP_PATH" >> $GITHUB_ENV
          else
            echo "✗ Backup failed"
            exit 1
          fi

      - name: Compress backup
        if: github.event.inputs.compress != 'false'
        run: |
          BACKUP_NAME="backup-production-${{ steps.timestamp.outputs.timestamp }}"
          BACKUP_PATH="${{ env.BACKUP_DIR }}/${BACKUP_NAME}"

          echo "Compressing backup..."
          tar -czf "${BACKUP_PATH}.tar.gz" -C "${{ env.BACKUP_DIR }}" "$BACKUP_NAME"

          if [ $? -eq 0 ]; then
            echo "✓ Compression completed"
            rm -rf "$BACKUP_PATH"
            echo "backup_path=${BACKUP_PATH}.tar.gz" >> $GITHUB_ENV
          else
            echo "✗ Compression failed"
            exit 1
          fi

      - name: Get backup size
        id: size
        run: |
          SIZE=$(du -h "${{ env.backup_path }}" | cut -f1)
          echo "Backup size: $SIZE"
          echo "size=$SIZE" >> $GITHUB_OUTPUT

      # Upload to S3 (uncomment and configure if using AWS)
      # - name: Configure AWS credentials
      #   if: github.event.inputs.upload != 'false'
      #   uses: aws-actions/configure-aws-credentials@v4
      #   with:
      #     aws-access-key-id: ${{ secrets.AWS_ACCESS_KEY_ID }}
      #     aws-secret-access-key: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
      #     aws-region: ${{ secrets.AWS_REGION }}
      #
      # - name: Upload to S3
      #   if: github.event.inputs.upload != 'false'
      #   run: |
      #     aws s3 cp "${{ env.backup_path }}" \
      #       "s3://${{ secrets.S3_BUCKET }}/backups/$(basename ${{ env.backup_path }})"
      #
      #     if [ $? -eq 0 ]; then
      #       echo "✓ Uploaded to S3"
      #     else
      #       echo "✗ Failed to upload to S3"
      #       exit 1
      #     fi

      # Upload to Google Cloud Storage (uncomment and configure if using GCS)
      # - name: Authenticate to Google Cloud
      #   if: github.event.inputs.upload != 'false'
      #   uses: google-github-actions/auth@v2
      #   with:
      #     credentials_json: ${{ secrets.GCP_CREDENTIALS }}
      #
      # - name: Upload to GCS
      #   if: github.event.inputs.upload != 'false'
      #   uses: google-github-actions/upload-cloud-storage@v2
      #   with:
      #     path: ${{ env.backup_path }}
      #     destination: ${{ secrets.GCS_BUCKET }}/backups/

      # Store as GitHub artifact (short-term storage, 90 days max)
      - name: Upload backup as artifact
        uses: actions/upload-artifact@v4
        with:
          name: mongodb-backup-${{ steps.timestamp.outputs.timestamp }}
          path: ${{ env.backup_path }}
          retention-days: 30
          compression-level: 0  # Already compressed if enabled

      - name: Create backup summary
        run: |
          echo "## Backup Summary" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          echo "- **Timestamp**: ${{ steps.timestamp.outputs.timestamp }}" >> $GITHUB_STEP_SUMMARY
          echo "- **Size**: ${{ steps.size.outputs.size }}" >> $GITHUB_STEP_SUMMARY
          echo "- **Compressed**: ${{ github.event.inputs.compress != 'false' }}" >> $GITHUB_STEP_SUMMARY
          echo "- **Uploaded**: ${{ github.event.inputs.upload != 'false' }}" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          echo "✅ Backup completed successfully" >> $GITHUB_STEP_SUMMARY

      # Optional: Send notification on failure
      # - name: Notify on failure
      #   if: failure()
      #   uses: slackapi/slack-github-action@v1.24.0
      #   with:
      #     payload: |
      #       {
      #         "text": "❌ Database backup failed!",
      #         "blocks": [
      #           {
      #             "type": "section",
      #             "text": {
      #               "type": "mrkdwn",
      #               "text": "❌ *Database Backup Failed*\n*Repository:* ${{ github.repository }}\n*Triggered by:* ${{ github.actor }}"
      #             }
      #           }
      #         ]
      #       }
      #   env:
      #     SLACK_WEBHOOK_URL: ${{ secrets.SLACK_WEBHOOK_URL }}

  # Optional: Clean up old backups from cloud storage
  cleanup-old-backups:
    name: Cleanup Old Backups
    runs-on: ubuntu-latest
    needs: backup-database
    if: always()

    steps:
      - name: Cleanup note
        run: |
          echo "## Backup Retention" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          echo "GitHub Actions artifacts are automatically deleted after 30 days." >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          echo "If using cloud storage (S3/GCS), configure lifecycle policies:" >> $GITHUB_STEP_SUMMARY
          echo "- Daily backups: 30 days" >> $GITHUB_STEP_SUMMARY
          echo "- Weekly backups: 90 days" >> $GITHUB_STEP_SUMMARY
          echo "- Monthly backups: 365 days" >> $GITHUB_STEP_SUMMARY

      # AWS S3 cleanup example (uncomment and configure)
      # - name: Configure AWS credentials
      #   uses: aws-actions/configure-aws-credentials@v4
      #   with:
      #     aws-access-key-id: ${{ secrets.AWS_ACCESS_KEY_ID }}
      #     aws-secret-access-key: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
      #     aws-region: ${{ secrets.AWS_REGION }}
      #
      # - name: Delete old backups from S3
      #   run: |
      #     # Delete backups older than 30 days
      #     aws s3 ls "s3://${{ secrets.S3_BUCKET }}/backups/" | \
      #       while read -r line; do
      #         createDate=$(echo $line | awk '{print $1" "$2}')
      #         createDate=$(date -d "$createDate" +%s)
      #         olderThan=$(date --date "30 days ago" +%s)
      #         if [[ $createDate -lt $olderThan ]]; then
      #           fileName=$(echo $line | awk '{print $4}')
      #           echo "Deleting old backup: $fileName"
      #           aws s3 rm "s3://${{ secrets.S3_BUCKET }}/backups/$fileName"
      #         fi
      #       done
